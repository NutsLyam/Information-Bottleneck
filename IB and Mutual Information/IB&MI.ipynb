{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "\n",
    "import pymorphy2\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import bisect\n",
    "from collections import namedtuple,defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import lxml.html\n",
    "import lxml.etree\n",
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "DocEntry = namedtuple('DocEntry',['doc_id','positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"news_base.db\") \n",
    "cursor = conn.cursor()\n",
    "\"\"\"создание тестовой выборки, чтобы проверить качетсво кластеризации\n",
    "\"\"\"\n",
    "\n",
    "d=[] #список документов для дальнейшей работы\n",
    "doc_id = 0\n",
    "\n",
    "stopWords = stopwords.words('russian')\n",
    "stopWords.extend(['что', 'это', 'так', 'вот', 'быть','й', 'как', 'в', '—', 'к', 'на','свой', 'который', 'е', 'также','год'])\n",
    "\n",
    "test_doc_to_clast = defaultdict(list)\n",
    "\n",
    "categories = ['Культура','Наука и техника','Спорт','Мир', 'Финансы','Бывший СССР']\n",
    "\"\"\" 0 - культура , 1 -наука, 3 - спорт\"\"\"\n",
    "for n_clast,  c in enumerate(categories):\n",
    "    sql = \"SELECT article FROM News where category = ? limit 1000\"\n",
    "    cursor.execute(sql,[c] )\n",
    "    \n",
    "    for t in cursor.fetchall()[:100]:\n",
    "        \n",
    "        test_doc_to_clast[n_clast].append(doc_id)\n",
    "        article = t[0]\n",
    "        d.append(article)\n",
    "        #print( n_clast, c, doc_id,article,\"\\n\")\n",
    "        doc_id+=1\n",
    "\n",
    "x = np.array(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" инвертированный индекс \n",
    "    используется для построения матриц вероятностей \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    words = (word for word in re.split('\\W+', text) if (len(word) >0 )&( analyzer.normal_forms(word)[0].lower() not in stopWords ))\n",
    "    lexems = (analyzer.normal_forms(word)[0] for word in words)\n",
    "    \n",
    "    return list(lexems)\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dict = defaultdict(list)\n",
    "        self.texts = dict()\n",
    "        \n",
    "    def add_document(self, text, doc_id):\n",
    "        self.texts[doc_id] = text\n",
    "        words = parse_text(text)\n",
    "        \n",
    "        \n",
    "        word_to_entry = defaultdict(lambda: [])\n",
    "        \n",
    "        for pos, word in enumerate(words):\n",
    "            \n",
    "            doc_entry = word_to_entry[word]\n",
    "            doc_entry.append(pos)\n",
    "            \n",
    "        for word, positions in word_to_entry.items():\n",
    "            postings = self.dict[word]\n",
    "            entry = DocEntry(doc_id, positions)\n",
    "            \n",
    "            i = bisect.bisect_left(postings, entry)\n",
    "        \n",
    "            postings.insert(i, entry)\n",
    "            \n",
    "    def get_postings(self, word):\n",
    "        return self.dict[word]\n",
    "    \n",
    "\n",
    "        \n",
    "\"\"\"Подсчет векторного произведения\"\"\"        \n",
    "def mult_single(x,y):\n",
    "    #conditions x>0 y>o\n",
    "    return x*y\n",
    "\n",
    "def mult_vect(x,y):\n",
    "    mult = np.vectorize(mult_single)\n",
    "    return np.sum(mult(x,y))\n",
    "\n",
    "\"\"\"Dkl\"\"\"\n",
    "\n",
    "def dkl_single(x,y):\n",
    "    if x>0 and y>0:\n",
    "     \n",
    "        return x*math.log2(x/y)\n",
    "    elif x==0: return 0.0\n",
    "    else: return math.inf\n",
    "\n",
    "def dkl(x,y):\n",
    "    sum_dkl = np.vectorize(dkl_single)\n",
    "    \n",
    "    return np.sum(sum_dkl(x,y))\n",
    "\n",
    "def div_matr(D, n, m,py_x,py_t) :\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            Div[i,j] = dkl(py_x[:,i],py_t.transpose()[:,j]) \n",
    "            \n",
    "    return Div\n",
    "    \n",
    "\"\"\"Djs\"\"\"\n",
    "def djs(x,y):\n",
    "    return 0.5 * dkl(x, (x+y)/2) + 0.5* dkl(y, (x+y)/2)\n",
    "\n",
    "def djs_vect(Djs,m,pt_x,pt_x_new ):\n",
    "    for i in range(m):\n",
    "        \n",
    "        Djs[i]= djs(pt_x[i,:], pt_x_new[i,:])\n",
    "    return Djs\n",
    "\n",
    "\"\"\"P(t)\"\"\"\n",
    "def f_pt(pt,m,px,pt_x):\n",
    "    for i in range(m):\n",
    "        pt[i] = mult_vect(px, pt_x[i,:])\n",
    "    return pt\n",
    "\n",
    "\"\"\"P(y|t)\"\"\"\n",
    "def f_py_t(py_t, m,n,pt_x,pxy,pt):\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            py_t[i,j] = mult_vect(pt_x[i,:],pxy[:,j])/pt[i]\n",
    "            \n",
    "    return py_t\n",
    "\"\"\"Выводит список из  n элементов, соответсвующих  \"\"\"\n",
    "def main_words(n, r):\n",
    "    l = []\n",
    "\n",
    "    for i in range(n):\n",
    "        l.append(r.argmax())\n",
    "        r[r.argmax()] = 0\n",
    "    return l\n",
    "\n",
    "\n",
    "        \n",
    "#show all numbers of docs from  cl,   \n",
    "def show_text_docs(cl_num):\n",
    "    show_main_words(cl_num)\n",
    "    print(\"\\n\")\n",
    "    docs = doc_clast[cl_num] # список документов кластера №cl_num\n",
    "    for num in docs:\n",
    "        print(\"№ документа: \" + str(num) + \" ТЕКСТ \"+ str(x[num]))\n",
    "    \n",
    "    \n",
    "    \n",
    "def foo_z(pt_x):\n",
    "    z = pt_x.sum(axis=0)\n",
    "    return z\n",
    "\n",
    "\n",
    "#w_d список документов со словом w\n",
    "#d_cl список документов из кластера t \n",
    "\n",
    "#w_d,  d_cl\n",
    "def f_N11( w_d,d_cl,n):\n",
    "    k= 0\n",
    "    \n",
    "    for d in range(n):\n",
    "        if (d in d_cl )& (d in w_d):\n",
    "            k+=1\n",
    "    return k\n",
    "#not w_d, w_cl\n",
    "def f_N01(w_d,w_cl,n):\n",
    "    k= 0\n",
    "    for d in range(n):\n",
    "        if (d in w_cl )& (d not in w_d):\n",
    "            k+=1\n",
    "    return k\n",
    "#not w_d, not w_cl\n",
    "def f_N00( w_d,w_cl,n):\n",
    "    k= 0\n",
    "    for d in range(n):\n",
    "        if (d not in w_cl )& (d not in w_d):\n",
    "            k+=1\n",
    "    return k\n",
    "\n",
    "#w_d,   not w_cl\n",
    "def f_N10( w_d,w_cl,n):\n",
    "    k= 0\n",
    "    for d in range(n):\n",
    "        if (d not in w_cl )& (d in w_d):\n",
    "            k+=1\n",
    "    return k\n",
    "\n",
    "def mutual_information(t):\n",
    "    MI = []\n",
    "    for word in (word_dict.items()):\n",
    "        w  = word[1]\n",
    "        \n",
    "        w_d=[]\n",
    "        for doc_id,pos in index.dict.get(w):\n",
    "            w_d.append(doc_id)\n",
    "        N = len(x)\n",
    "        N11 = f_N11(w_d,doc_clast.get(t),len(x))\n",
    "        N1 = len(w_d)\n",
    "        N10 = f_N10(w_d,doc_clast.get(t),len(x))\n",
    "        N0 = N - N1\n",
    "        #print(N, N1, N0)\n",
    "        N01 = f_N01(w_d,doc_clast.get(t),len(x))\n",
    "        N00 = f_N00(w_d,doc_clast.get(t),len(x))\n",
    "        if N10==0: N10=0.0001\n",
    "        if N01==0: N01=0.0001\n",
    "        if N11==0: N11=0.0001\n",
    "        if N00==0: N00=0.0001\n",
    "   \n",
    "        #print(N11, N01, N10, N00)\n",
    "        I = N11/N * math.log2(N*N11/(N1*N1)) + N01/N*math.log2(N * N01/(N0*N1)) +N10/N*math.log2(N * N10/(N0*N1))+N00/N*math.log2(N * N00/(N0*N0))\n",
    "        MI.append(I)\n",
    "    return MI\n",
    "    \n",
    "def main_words(n, r):\n",
    "    l = []\n",
    "    \n",
    "    for i in range(n): \n",
    "        maxi = r.argmax()\n",
    "        l.append((maxi,r[maxi]))\n",
    "        r[maxi] = 0\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#beta\n",
    "beta = 2.11\n",
    "\n",
    "#число кластеров\n",
    "m = 6\n",
    "#covergence parameter \n",
    "\n",
    "e = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "\"\"\"INPUT\"\"\"\n",
    "\n",
    "analyzer = pymorphy2.MorphAnalyzer()   \n",
    "\"\"\"Создание инвертированного индекса\"\"\"   \n",
    "index = InvertedIndex()\n",
    "\n",
    "for i, line in enumerate(x):\n",
    "        doc_id = i\n",
    "        text = line\n",
    "        index.add_document(text,doc_id)\n",
    "        \n",
    "#словарь слов: ключ - номер индекса , значение -словао\n",
    "word_dict = {}\n",
    "for i, word in enumerate(index.dict.keys()):\n",
    "    word_dict[i] = word\n",
    "\n",
    "\"\"\"общее число слов в коллекции\"\"\"\n",
    "summa = 0\n",
    "for i in x:\n",
    "    #print(len(word_tokenize(i)))\n",
    "    summa+=len(word_tokenize(i))\n",
    "\n",
    "\"\"\"P(x,y)\"\"\"    \n",
    "pxy = np.zeros((len(x), len(index.dict.keys())))\n",
    "for i, word in enumerate(index.dict.keys()):\n",
    "    a = index.dict.get(word)\n",
    "    for doc_id , postings in a:\n",
    "        pxy[doc_id,i] = len(postings)/summa\n",
    "\"\"\"print(\"pxy\")\n",
    "print(pxy)\"\"\"     \n",
    "        \n",
    "\"\"\"INITIALIZaTION\"\"\" \n",
    "\n",
    "\"\"\"P(t|x)\"\"\"\n",
    "pt_x = np.random.rand(m, len(x))\n",
    "\n",
    "z = pt_x.sum(axis=0)\n",
    "pt_x = pt_x/z\n",
    "#print(pt_x)\n",
    "    \n",
    "\"\"\"задание вероятности P(x)\"\"\"    \n",
    "px  = np.zeros((x.size))\n",
    "\n",
    "for n, i in enumerate(x):\n",
    "    b = len(word_tokenize(i))\n",
    "    #print(i, b, n)\n",
    "    px[n] = b/summa\n",
    "#print(\"px\")\n",
    "#print(px)\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"P(t)\"\"\"\n",
    "pt = np.zeros(m)\n",
    "for i in range(m):\n",
    "    pt[i] = mult_vect(px,pt_x[i,:])\n",
    "    \n",
    "\"\"\"P(y|t)\"\"\"\n",
    "py_t = np.zeros((m,len(index.dict.keys())) )\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(len(index.dict.keys())):\n",
    "        py_t[i,j] = mult_vect(pt_x[i,:],pxy[:,j])/pt[i]\n",
    "        \n",
    "\n",
    "\"\"\"P(y|x)\"\"\"\n",
    "py_x = np.zeros((len(index.dict.keys()),len(x)))\n",
    "\n",
    "\n",
    "word_index=[]\n",
    "for i, word in enumerate(index.dict.keys()):\n",
    "    a = index.dict.get(word)\n",
    "    word_index.append((i,word))\n",
    "    for doc_id , postings in a:\n",
    "        py_x[i,doc_id] = len(postings)/len(index.texts[doc_id].split())\n",
    "        \n",
    "\n",
    "      \n",
    "        \"\"\"LOOP\"\"\"\n",
    "#print(\"y_x\")\n",
    "#print(py_x) \n",
    "Div = np.zeros((len(x), m)) #init div   \n",
    "Div = div_matr(Div, len(x),m,py_x,py_t)\n",
    "#new P(t|x)\n",
    "#z = f_z(pt,py_x,py_t,beta)\n",
    "\n",
    "pt_x_new =pt * np.exp(-beta * Div)\n",
    "pt_x_new = pt_x_new.transpose()\n",
    "#print(\"pt_x_new\")\n",
    "#print(pt_x_new)\n",
    "z = foo_z(pt_x_new)\n",
    "#print(\"z\")\n",
    "#print(z)\n",
    "pt_x_new = pt_x_new/z\n",
    "#print(\"pt_x_new\")\n",
    "#print(pt_x_new)\n",
    "\n",
    "#init Djs\n",
    "Djs = np.zeros(m)\n",
    "Djs = djs_vect(Djs,m,pt_x,pt_x_new )\n",
    "\n",
    "k = 0\n",
    "\"\"\"print(\"pt_x\")\n",
    "print(pt_x)\n",
    "print(\"pt\")\n",
    "print(pt)\n",
    "print(\"py_t\")\n",
    "print(py_t)\n",
    "print(\"DJS\")\n",
    "print(Djs)\"\"\"\n",
    "\n",
    "while np.sum(Djs)/len(Djs)>=e:\n",
    "    print(k)\n",
    "    #переопрделяем \n",
    "    pt_x = pt_x_new \n",
    "    \"\"\"print(\"pt_x\")\n",
    "    print(pt_x)\"\"\"\n",
    "    #P(t)    \n",
    "    pt = f_pt(pt,m,px,pt_x)\n",
    "    \"\"\"print(\"pt\")\n",
    "    print(pt)\"\"\"\n",
    "    \n",
    "    #P(y|t)\n",
    "    py_t = f_py_t(py_t, m,len(index.dict.keys()),pt_x,pxy,pt) \n",
    "    \"\"\"print(py_t)\n",
    "    print(py_t)\"\"\"\n",
    "                  \n",
    "    #P(t|x) new\n",
    "    pt_x_new = pt * np.exp(-beta * div_matr(Div, len(x),m,py_x,py_t)) \n",
    "    \"\"\"print('pt_new')\n",
    "    print(pt_x_new)\"\"\"\n",
    "    pt_x_new = pt_x_new.transpose()\n",
    "    z = foo_z(pt_x_new)\n",
    "    pt_x_new = pt_x_new/z\n",
    "    \"\"\"print(\"pt_x_new\")\n",
    "    print(pt_x_new)\"\"\"\n",
    "       \n",
    "    Djs = djs_vect(Djs,m,pt_x,pt_x_new )\n",
    "    #print(\"DJS\")\n",
    "    #print(Djs)\n",
    "    k+=1\n",
    "\n",
    "# вектор с номерами кластеров, к которому документ предложит( вероятность больше всего)\n",
    "vec = np.argmax(pt_x_new, axis=0)\n",
    "\n",
    "#словарь: ключ - номер кластера, значение - номер документа\n",
    "doc_clast = defaultdict(list)\n",
    "for i,value in enumerate(vec):\n",
    "        doc_clast[value].append(i)\n",
    "\n",
    "        \n",
    "#СЛОВАРЬ: ключ - номер кластера, значение - список и n самых важных слов\n",
    "\n",
    "clasters_main_words = {}\n",
    "n = 50\n",
    "py_t_copy = py_t.copy()\n",
    "for i, row in enumerate(py_t_copy):  \n",
    "    \n",
    "    clasters_main_words[i] = main_words(n, row) \n",
    "    #print(clasters_main_words[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "MI = mutual_information(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MainWords=[]\n",
    "\n",
    "for i,weight in main_words(1000, np.array(MI.copy())):\n",
    "    MainWords.append((i,word_dict[i],weight))\n",
    "MainWords \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['главный тренер мадридского «реала» зинедин зидан перед финальным матчем лиги чемпионов с туринским «ювентусом» решил мотивировать футболистов, показав им фильм. об этом сообщает marca.', 'французский специалист выбрал картину 2006 года режиссера зака снайдера «300 спартанцев». игрокам будут продемонстрированы самые яркие фрагменты фильма. тренер хочет, чтобы его футболисты прониклись духом спартанского царя леонида и его воинов. по его мнению, просмотр киноленты будет эффективнее мотивационных речей.', 'фильм основан на комиксах фрэнка миллера. речь в картине идет о 300 спартанцах, которые остановили многотысячную армию, ведомую персидским царем ксерксом.', 'финальная встреча лиги чемпионов между «реалом» и «ювентусом» пройдет в субботу, 3 июня. игра состоится в кардиффе на стадионе «миллениум». матч начнется в 21:45 по московскому времени.']\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)\n",
    "doc_clast[0]\n",
    "x[208]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "#подсчет статистики\n",
    "l2  = test_doc_to_clast[2] \n",
    "l1 = doc_clast[5]\n",
    "\n",
    "result=list(set(l1) & set(l2))\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#словарь слово ключ : индекс слова, значение : вес\n",
    "mi_dict={}\n",
    "for i, mi in enumerate(MI):\n",
    "    mi_dict[i] = mi\n",
    "    \n",
    "    \n",
    "def sent_weight(l):  #возвращает вес предложения\n",
    "    weight = 0\n",
    "    for word in l:\n",
    "        weight+=mi_dict[new_word_dict[word]]\n",
    "    return weight\n",
    "\n",
    "# переопределение словаря, теперь ключ: слово, значение :индекс\n",
    "new_word_dict = {}\n",
    "for i, word in enumerate(index.dict.keys()):\n",
    "    new_word_dict[word] = i  \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# считаем веса каждого предложения новости\n",
    "data = x[208]\n",
    "sentences = sent_tokenize(data)\n",
    "sentence_weight=[]\n",
    "for i,sent in enumerate(sentences):\n",
    "    parsed_text = parse_text(sent)\n",
    "    sentence_weight.append(sent_weight(parsed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.782580727012224\n",
      "1.44102062952326\n",
      "6.900902070518959\n",
      "2.6685637708610797\n",
      "8.390562526684695\n",
      "4.371469147926859\n",
      "3.4736381993567758\n",
      "10.87011029131903\n",
      "4.079230897220543\n",
      "3.213912740981539\n",
      "1.6384854554490085\n"
     ]
    }
   ],
   "source": [
    "#добавить сортировку \n",
    "for i in sentence_weight:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['главный тренер мадридского «реала» зинедин зидан перед финальным матчем лиги чемпионов с туринским «ювентусом» решил мотивировать футболистов, показав им фильм.\n",
      "', 'французский специалист выбрал картину 2006 года режиссера зака снайдера «300 спартанцев».\n",
      "тренер хочет, чтобы его футболисты прониклись духом спартанского царя леонида и его воинов.\n",
      "речь в картине идет о 300 спартанцах, которые остановили многотысячную армию, ведомую персидским царем ксерксом.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(sentences[2])\n",
    "print(sentences[4])\n",
    "print(sentences[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14949203,  0.2258139 ,  0.09211367,  0.21299509,  0.13915805,\n",
       "        0.18042726])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#распределение документов по кластерам\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.41317602e-04,   4.87777507e-01,   1.12109934e-06,\n",
       "         1.64097961e-01,   3.01598758e-01,   4.63833358e-02])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_x[:,340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

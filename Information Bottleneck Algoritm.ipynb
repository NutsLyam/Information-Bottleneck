{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "\n",
    "import pymorphy2\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import bisect\n",
    "from collections import namedtuple,defaultdict\n",
    "import numpy as np\n",
    "\n",
    "DocEntry = namedtuple('DocEntry',['doc_id','positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data\n",
    "d = [\"cat dog cat home cat animal\", \"duck cow home cow\", \"dog hunter duck duck dog gun river\", \"crocodile tiger is cat animal \", \"animal crocodile crocodile river\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" инвертированный индекс \n",
    "    используется для построения матриц вероятностей \n",
    "    \"\"\"\n",
    "analyzer = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def parse_text(text):\n",
    "    words = (word for word in re.split('\\W+', text) if len(word) >0)\n",
    "    lexems = (analyzer.normal_forms(word)[0] for word in words)\n",
    "    \n",
    "    return list(lexems)\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dict = defaultdict(list)\n",
    "        self.texts = dict()\n",
    "        \n",
    "    def add_document(self, text, doc_id):\n",
    "        self.texts[doc_id] = text\n",
    "        words = parse_text(text)\n",
    "        \n",
    "        \n",
    "        word_to_entry = defaultdict(lambda: [])\n",
    "        \n",
    "        for pos, word in enumerate(words):\n",
    "            \n",
    "            doc_entry = word_to_entry[word]\n",
    "            doc_entry.append(pos)\n",
    "            \n",
    "        for word, positions in word_to_entry.items():\n",
    "            postings = self.dict[word]\n",
    "            entry = DocEntry(doc_id, positions)\n",
    "            \n",
    "            i = bisect.bisect_left(postings, entry)\n",
    "        \n",
    "            postings.insert(i, entry)\n",
    "            \n",
    "    def get_postings(self, word):\n",
    "        return self.dict[word]\n",
    "    \n",
    "    \n",
    "\"\"\"Создание инвертированного индекса\"\"\"   \n",
    "index = InvertedIndex()\n",
    "\n",
    "for i, line in enumerate(x):\n",
    "        doc_id = i\n",
    "        text = line\n",
    "        index.add_document(text,doc_id)\n",
    "        \n",
    "\"\"\"Подсчет векторного произведения\"\"\"        \n",
    "def mult_single(x,y):\n",
    "    #conditions x>0 y>o\n",
    "    return x*y\n",
    "\n",
    "def mult_vect(x,y):\n",
    "    mult = np.vectorize(mult_single)\n",
    "    return np.sum(mult(x,y))\n",
    "\n",
    "\"\"\"Dkl\"\"\"\n",
    "\n",
    "def dkl_single(x,y):\n",
    "    if x>0 and y>0:\n",
    "     \n",
    "        return x*math.log2(x/y)\n",
    "    elif x==0: return 0.0\n",
    "    else: return math.inf\n",
    "\n",
    "def dkl(x,y):\n",
    "    sum_dkl = np.vectorize(dkl_single)\n",
    "    \n",
    "    return np.sum(sum_dkl(x,y))\n",
    "\n",
    "def div_matr(D, n, m,py_x,py_t) :\n",
    "    for i in range(len(x)):\n",
    "        for j in range(m):\n",
    "            Div[i,j] = dkl(py_x[:,i],py_t.transpose()[:,j]) \n",
    "            \n",
    "    return Div\n",
    "    \n",
    "\"\"\"Djs\"\"\"\n",
    "def djs(x,y):\n",
    "    return 0.5 * dkl(x, (x+y)/2) + 0.5* dkl(y, (x+y)/2)\n",
    "\n",
    "def djs_vect(Djs,m,pt_x,pt_x_new ):\n",
    "    for i in range(m):\n",
    "        \n",
    "        Djs[i]= djs(pt_x[i,:], pt_x_new[i,:])\n",
    "    return Djs\n",
    "\n",
    "\"\"\"P(t)\"\"\"\n",
    "def f_pt(pt,m,px,pt_x):\n",
    "    for i in range(m):\n",
    "        pt[i] = mult_vect(px, pt_x[i,:])\n",
    "    return pt\n",
    "\n",
    "\"\"\"P(y|t)\"\"\"\n",
    "def f_py_t(py_t, m,n,pt_x,pxy,pt):\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            py_t[i,j] = mult_vect(pt_x[i,:],pxy[:,j])/pt[i]\n",
    "            \n",
    "    return py_t\n",
    "\"\"\"Выводит список из  n элементов, соответсвующих  \"\"\"\n",
    "def main_words(n, r):\n",
    "    l = []\n",
    "\n",
    "    for i in range(n):\n",
    "        l.append(r.argmax())\n",
    "        r[r.argmax()] = 0\n",
    "    return l\n",
    "\n",
    "#словарь слов: ключ - номер индекса , значение -словао\n",
    "word_dict = {}\n",
    "for i, word in enumerate(index.dict.keys()):\n",
    "    word_dict[i] = word\n",
    "    \n",
    "# создает список из n самых важных слов для кластера   \n",
    "def main_words(n, r):\n",
    "    l = []\n",
    "\n",
    "    for i in range(n):\n",
    "        l.append(r.argmax())\n",
    "        r[r.argmax()] = 0\n",
    "    return l\n",
    "\n",
    "#печатает самых важные слова для кластера\n",
    "#cl_num - number of cluster\n",
    "def show_main_words(cl_num):\n",
    "    for w in clasters_main_words.get(cl_num):\n",
    "        print(\"Самые важные слова кластера №\" + str(cl_num) + \": \" + str(word_dict.get(w)))\n",
    "        \n",
    "#show all numbers of docs from  cl,   \n",
    "def show_text_docs(cl_num):\n",
    "    show_main_words(cl_num)\n",
    "    print(\"\\n\")\n",
    "    docs = doc_clast[cl_num] # список документов кластера №cl_num\n",
    "    for num in docs:\n",
    "        print(\"№ документа: \" + str(num) + \" ТЕКСТ \"+ str(x[num]))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"INPUT\"\"\"\n",
    "#beta\n",
    "beta = 1.2\n",
    "\n",
    "#число кластеров\n",
    "m = 2\n",
    "\n",
    "#covergence parameter \n",
    "\n",
    "e = 0.1\n",
    "\n",
    "\"\"\"общее число слов в коллекции\"\"\"\n",
    "summa = 0\n",
    "for i in x:\n",
    "    #print(len(word_tokenize(i)))\n",
    "    summa+=len(word_tokenize(i))\n",
    "\n",
    "\"\"\"P(x,y)\"\"\"    \n",
    "pxy = np.zeros((len(x), len(index.dict.keys())))\n",
    "for i, word in enumerate(index.dict.keys()):\n",
    "    a = index.dict.get(word)\n",
    "    for doc_id , postings in a:\n",
    "        pxy[doc_id,i] = len(postings)/summa\n",
    "        \n",
    "        \n",
    "\"\"\"INITIALIZaTION\"\"\" \n",
    "\n",
    "\"\"\"P(t|x)\"\"\"\n",
    "pt_x = np.random.rand(m, len(x))\n",
    "    \n",
    "\"\"\"задание вероятности P(x)\"\"\"    \n",
    "px  = np.zeros((x.size))\n",
    "\n",
    "for n, i in enumerate(x):\n",
    "    b = len(word_tokenize(i))\n",
    "    #print(i, b, n)\n",
    "    px[n] = b/summa\n",
    "    \n",
    "\"\"\"random P(t|x)\"\"\"\n",
    "pt_x = np.random.rand(m, len(x))\n",
    "\n",
    "\"\"\"P(t)\"\"\"\n",
    "pt = np.zeros(m)\n",
    "for i in range(m):\n",
    "    pt[i] = mult_vect(px,pt_x[i,:])\n",
    "    \n",
    "\"\"\"P(y|t)\"\"\"\n",
    "py_t = np.zeros((m,len(index.dict.keys())) )\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(len(index.dict.keys())):\n",
    "        py_t[i,j] = mult_vect(pt_x[i,:],pxy[:,j])/pt[i]\n",
    "        \n",
    "\n",
    "\"\"\"P(y|x)\"\"\"\n",
    "py_x = np.zeros((len(index.dict.keys()),len(x)))\n",
    "word_index=[]\n",
    "for i, word in enumerate(index.dict.keys()):\n",
    "    a = index.dict.get(word)\n",
    "    word_index.append((i,word))\n",
    "    for doc_id , postings in a:\n",
    "        py_x[i,doc_id] = len(postings)/len(index.texts[doc_id].split())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"LOOP\"\"\"\n",
    "\n",
    "Div = np.zeros((len(x), m)) #init div   \n",
    "Div = div_matr(Div, len(x),m,py_x,py_t)\n",
    "\n",
    "#new P(t|x)\n",
    "pt_x_new =pt * np.exp(-beta * Div)\n",
    "pt_x_new = pt_x_new.transpose()\n",
    "\n",
    "#init Djs\n",
    "Djs = np.zeros(m)\n",
    "Djs = djs_vect(Djs,m,pt_x,pt_x_new )\n",
    "\n",
    "k = 0\n",
    "\n",
    "while np.sum(Djs)/len(Djs)>=e:\n",
    "    #переопрделяем \n",
    "    pt_x = pt_x_new    \n",
    "    #P(t)    \n",
    "    pt = f_pt(pt,m,px,pt_x)\n",
    "    \n",
    "    #P(y|t)\n",
    "    py_t = f_py_t(py_t, m,len(index.dict.keys()),pt_x,pxy,pt) \n",
    "                  \n",
    "    #P(t|x) new\n",
    "    pt_x_new = pt * np.exp(-beta * div_matr(Div, len(x),m,py_x,py_t)) \n",
    "    pt_x_new = pt_x_new.transpose()\n",
    "       \n",
    "    Djs = djs_vect(Djs,m,pt_x,pt_x_new )\n",
    "    k+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03095168,  0.00053149,  0.00572824,  0.04532262,  0.01729208],\n",
       "       [ 0.01769143,  0.0007848 ,  0.0014737 ,  0.04993393,  0.02036563]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вектор с номерами кластеров, к которому документ предложит( вероятность больше всего)\n",
    "vec = np.argmax(pt_x_new, axis=0)\n",
    "\n",
    "#словарь: ключ - номер кластера, значение - номер документа\n",
    "doc_clast = defaultdict(list)\n",
    "for i,value in enumerate(vec):\n",
    "        doc_clast[value].append(i)\n",
    "\n",
    "        \n",
    "#СЛОВАРЬ: ключ - номер кластера, значение - список и n самых важных слов\n",
    "\n",
    "clasters_main_words = {}\n",
    "n = 2\n",
    "py_t_copy = py_t.copy()\n",
    "for i, row in enumerate(py_t_copy):  \n",
    "    clasters_main_words[i] = main_words(n, row) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самые важные слова кластера №0: cat\n",
      "Самые важные слова кластера №0: animal\n",
      "\n",
      "\n",
      "№ документа: 0 ТЕКСТ cat dog cat home cat animal\n",
      "№ документа: 2 ТЕКСТ dog hunter duck duck dog gun river\n"
     ]
    }
   ],
   "source": [
    "show_text_docs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самые важные слова кластера №1: crocodile\n",
      "Самые важные слова кластера №1: cat\n",
      "№ документа: 1 ТЕКСТ duck cow home cow\n",
      "№ документа: 3 ТЕКСТ crocodile tiger is cat animal \n",
      "№ документа: 4 ТЕКСТ animal crocodile crocodile river\n"
     ]
    }
   ],
   "source": [
    "show_text_docs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
